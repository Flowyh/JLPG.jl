#=== START OF ENVIRONEMNT SETUP ===#
{{{:env}}}
#===   END OF ENVIRONEMNT SETUP ===#

#=== START OF CODE BLOCKS DEFINED IN LEXER (INSERTED IN ORDER OF DEFINITION)===#
{{#:codeblocks}}
{{{.}}}
{{^.[end]}}

{{/.[end]}}
{{/:codeblocks}}
#===   END OF CODE BLOCKS DEFINED IN LEXER (INSERTED IN ORDER OF DEFINITION)===#

#=== START OF TOKENS RETURNED BY LEXER ACTIONS ===#
{{#:tokens}}
#<<< DECL __LEX__{{:name}} >>>#
struct __LEX__{{:name}} <: LexerToken
  symbol::Symbol
  values::Dict
  file_pos::String
end

function __LEX__{{:name}}(;{{#arguments}}{{:name}}::{{:type}}, {{/arguments}})::__LEX__{{:name}}
  return __LEX__{{:name}}(
    Symbol(uppercase(raw"{{:name}}")),
    Dict({{#arguments}}:{{:name}} => {{:name}},{{/arguments}}),
    __LEX__file_pos_before_match()
  )
end
#<<< EODL __LEX__{{:name}} >>>#
{{^.[end]}}

{{/.[end]}}
{{/:tokens}}

#<<< DECL __LEX__ END OF INPUT >>>#
struct __LEX__EOI <: LexerToken
  symbol::Symbol
  values::Dict
  file_pos::String
end

function __LEX__EOI()::__LEX__EOI
  return __LEX__EOI(
    END_OF_INPUT,
    Dict(),
    ""
  )
end
#<<< EODL __LEX__ END OF INPUT >>>#
#===   END OF TOKENS RETURNED BY LEXER ACTIONS ===#

#=== START OF __LEX__ ACTIONS ===#
{{#:actions}}
#<<< {{{:pattern}}} >>>#
function __LEX__action{{#:counter}}{{/:counter}}()::Union{LexerToken, Any}
  {{{:body}}}
end

{{/:actions}}{{#:reset_counter}}{{/:reset_counter}}
#<<< __LEX__ PATTERN TO ACTION FUNCTION MAPPINGS >>>#
const __LEX__PATTERN_TO_ACTION = Dict(
  {{#:actions}}
  r"{{{:pattern}}}" => __LEX__action{{#:counter}}{{/:counter}}{{^.[end]}}, {{/.[end]}}
  {{/:actions}}
)
#===   END OF ACTIONS ===#

#=== START OF TOKENIZE LOOP ===#
const __LEX__ACTION_PATTERNS = [
  {{#:actions}}
  r"{{{:pattern}}}"{{^.[end]}}, {{/.[end]}}
  {{/:actions}}
]

{{{:tokenize}}}
#===   END OF TOKENIZE LOOP ===#

#=== START OF MAIN FUNCTION ===#
{{{:main}}}
#===   END OF MAIN FUNCTION ===#

#=== START OF ENVIRONEMNT SETUP ===#
{{{:env}}}
#===   END OF ENVIRONEMNT SETUP ===#

#=== START OF CODE BLOCKS DEFINED IN LEXER (INSERTED IN ORDER OF DEFINITION)===#
{{#:codeblocks}}
{{{.}}}
{{^.[end]}}

{{/.[end]}}
{{/:codeblocks}}
#===   END OF CODE BLOCKS DEFINED IN LEXER (INSERTED IN ORDER OF DEFINITION)===#

#=== START OF TOKENS RETURNED BY LEXER ACTIONS ===#
{{#:tokens}}
#<<<: DECL {{:name}} :>>>#
struct {{:name}} <: LexerToken
  tag::Symbol
  values::Dict
end

function {{:name}}(;{{#arguments}}{{:name}}::{{:type}}, {{/arguments}})::{{:name}}
  return {{:name}}(
    :{{:name}},
    Dict({{#arguments}}:{{:name}} => {{:name}},{{/arguments}})
  )
end
#<<<: EODL {{:name}} :>>>#
{{^.[end]}}

{{/.[end]}}
{{/:tokens}}
#===   END OF TOKENS RETURNED BY LEXER ACTIONS ===#

#=== START OF ACTIONS ===#
{{#:actions}}
#<<<: {{{:pattern}}} >>>#
function action{{#:counter}}{{/:counter}}()::Union{LexerToken, Any}
  {{{:body}}}
end

#<<< PATTERN TO ACTION FUNCTION MAPPINGS >>>#{{/:actions}}{{#:reset_counter}}{{/:reset_counter}}
const PATTERN_TO_ACTION = Dict(
  {{#:actions}}
  r"{{{:pattern}}}" => action{{#:counter}}{{/:counter}}{{^.[end]}}, {{/.[end]}}
  {{/:actions}}
)
#===   END OF ACTIONS ===#

#=== START OF TOKENIZE LOOP ===#
const ACTION_PATTERNS = [
  {{#:actions}}
  r"{{{:pattern}}}"{{^.[end]}}, {{/.[end]}}
  {{/:actions}}
]

function tokenize(txt::String)::Vector{LexerToken}
  @debug "<<<<<: START OF TOKENIZE :>>>>>"
  tokens::Vector{LexerToken} = []
  cursor::Int = 1
  while cursor <= length(txt)
    did_match::Bool = false
    for pattern in ACTION_PATTERNS
      matched = findnext(pattern, txt, cursor)
      if matched === nothing || matched.start != cursor
        continue
      end
      @debug "New match of length $(length(matched)) found: $(txt[matched])"
      __LEX__at_endcurrent_match(txt[matched])

      token = PATTERN_TO_ACTION[pattern]()
      if token isa LexerToken
        @debug "New token has been created: $token"
        push!(tokens, token)
      end

      did_match = true
      cursor += length(matched)
      break
    end

    if !did_match
      error("Syntax error, cannot match remaining text: $(txt[cursor:end])")
    end
  end

  @debug "<<<<<:   END OF TOKENIZE :>>>>>"
  return tokens
end
#===   END OF TOKENIZE LOOP ===#

#=== START OF MAIN FUNCTION ===#
function main()
  # If the program is run directly, run the main loop
  # Otherwise read path from first argument
  tokens = nothing
  if length(ARGS) == 0
    txt::String = read(stdin, String)
    tokens = tokenize(txt)
  elseif ARGS[1] == "-h" || ARGS[1] == "--help"
    println("Usage: $(PROGRAM_FILE) [path]")
  elseif !isfile(ARGS[1])
    error("File \"$(ARGS[1])\" does not exist")
  else
    txt = ""
    open(ARGS[1]) do file
      txt = read(file, String)
    end
    tokens = tokenize(txt)
  end
  @debug "<<<<<: LEXER OUTPUT :>>>>>"
  @debug "Output tokens: $tokens"

  return __LEX__at_end()
end

if abspath(PROGRAM_FILE) == @__FILE__
  return main()
end
#===   END OF MAIN FUNCTION ===#

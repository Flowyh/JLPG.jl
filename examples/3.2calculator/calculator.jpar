%option LALR

%{
abstract type Operand end
add(a::Operand, b::Operand)::Operand = a + b
sub(a::Operand, b::Operand)::Operand = a - b
mul(a::Operand, b::Operand)::Operand = a * b
div(a::Operand, b::Operand)::Operand = a / b

struct GF{P} <: Operand
  value::Int
  GF{P}(value::Int) where {P} = new(mod(value, P))
end
(Base.:+)(a::GF{P}, b::GF{P}) where P = GF{P}(a.value + b.value)
(Base.:-)(a::GF{P}, b::GF{P}) where P = GF{P}(a.value - b.value)
(Base.:*)(a::GF{P}, b::GF{P}) where P = GF{P}(a.value * b.value)
(Base.:/)(a::GF{P}, b::GF{P}) where P = GF{P}(a.value * invmod(b.value, P))
Base.show(io::IO, a::GF{P}) where P = print(io, "GaloisField{order=$P}($(a.value))")

FIELD_PRIME::Int = 7
%}

#= Lexer token definitions =#
%token MULTIPLY "*"
%token DIVIDE "/"
%token ADD "+"
%token SUBTRACT "-"
%token LPAREN "("
%token RPAREN ")"
%token NUMBER

#= Returned types =#
%type <GF{FIELD_PRIME}> e
%type <GF{FIELD_PRIME}> t
%type <GF{FIELD_PRIME}> f

%%
#= Productions =#
%start s
s -> e         :{ println($1)                    }:
e -> e "+" t   :{ $$ = add($1, $3)               }:
   | e "-" t   :{ $$ = sub($1, $3)               }:
   | t         :{ $$ = $1                        }:
t -> t "*" f   :{ $$ = mul($1, $3)               }:
   | t "/" f   :{ $$ = div($1, $3)               }:
   | f         :{ $$ = $1                        }:
f -> NUMBER    :{ $$ = GF{FIELD_PRIME}($1.value) }:
   | "(" e ")" :{ $$ = $2                        }:

%%

function __PAR__usage()
  println("Usage: $(PROGRAM_FILE) [source file] [order]")
end

function __PAR__main()
  if length(ARGS) != 2
    return __PAR__usage()
  elseif ARGS[1] == "-h" || ARGS[1] == "--help"
    return __PAR__usage()
  elseif !isfile(ARGS[1])
    error("File \"$(ARGS[1])\" does not exist")
  else
    try
      global FIELD_PRIME = parse(Int, ARGS[2])
    catch e
      error("Invalid order argument, must be an integer, got: \"$(ARGS[2])\"")
    end

    txt = ""
    open(ARGS[1]) do file
      txt = read(file, String)
      __LEX__bind_cursor(Cursor(txt; source=ARGS[1]))
    end

    tokens = nothing
    try
      tokens = __LEX__tokenize()
    catch e
      e = ErrorException(replace(e.msg, r"\n       " => "\n"))
      @error "Error while tokenizing input" exception=(e, catch_backtrace())
      exit(1)
    end

    try
      __PAR__simulate(tokens)
    catch e
      if e isa ErrorException
        e = ErrorException(replace(e.msg, r"\n       " => "\n"))
        @error "Error while parsing tokens" exception=(e, catch_backtrace())
        exit(1)
      end
      @error "Error while parsing tokens" exception=(e, catch_backtrace())
    end
  end

  return __PAR__at_end()
end
